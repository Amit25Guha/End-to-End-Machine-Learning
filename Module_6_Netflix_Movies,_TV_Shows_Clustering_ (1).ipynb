{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - End to End Machine Learning\n",
        "\n",
        "Netflix-Movies-and-TV-Shows-Clustering."
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "\n",
        "NAME - AMIT GUHA\n",
        "\n",
        "Email - aguha535@gmail.com\n",
        "\n",
        "Video Presentation Link -"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix Inc. is an American media company based in Los Gatos, California. Founded in 1997 by Reed Hastings and Marc Randolph in Scotts Valley, California, it operates the over-the-top subscription video on-demand service Netflix brand, which includes original films and television series commissioned or acquired by the company, and third-party content licensed from other distributors.\n",
        "\n",
        "This dataset consists of tv shows and movies available on Netflix. The dataset is collected from Flexible which is a third-party Netflix search engine. Netflix movies and TV shows clustering is a data analysis and machine learning technique that Netflix uses to group its content into similar categories. This technique involves analyzing the various characteristics of each title, such as genre, cast, and plot, and using algorithms to identify patterns and similarities. In essence, it's a set of algorithms using machine learning to analyze user data and movie ratings. To make it more effective, Netflix has set up 1,300 recommendation clusters based on users viewing preferences. Netflix's target market is young, tech-savvy users and anyone with digital connectivity. The audience of Netflix is from diverse age groups and demographics. However, most of the audience are teenagers, college-goers, entrepreneurs, working professionals, etc. Netflix's target consumers are divided into segments based on demographics, behavioural intents, and psychographic segmentation. Like most licensing agreements, the deal is structured in a traditional form, whereby Netflix pays for each film determined by rate cards on a sliding scale by each title's domestic or worldwide box office receipts. Netflix uses machine learning and algorithms to help break viewers' preconceived notions and find shows that they might not have initially chosen. To do this, it looks at nuanced threads within the content, rather than relying on broad genres to make its predictions."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "#for nlp\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_path = \"/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\"\n",
        "df = pd.read_csv(working_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info(memory_usage = 'deep')"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "SrN7dRpsX89Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Defining Data Info All\n",
        "def DataInfoAll(df):\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    print(\"-\"*125)\n",
        "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
        "    summary = summary.reset_index()\n",
        "    summary['Name'] = summary['index']\n",
        "    summary = summary[['Name','dtypes']]\n",
        "    summary['Missing'] = df.isnull().sum().values\n",
        "    summary['Uniques'] = df.nunique().values\n",
        "    summary['First Value'] = df.iloc[0].values\n",
        "    summary['Second Value'] = df.iloc[1].values\n",
        "    return summary\n",
        "DataInfoAll(df)"
      ],
      "metadata": {
        "id": "kAzFdRzoX-VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df_duplicate = df[df.duplicated()]\n",
        "print(\"Let's print all the duplicated rows as a dataframe\")       # No duplicate values present in this dataset.\n",
        "df_duplicate"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "NaN_Checker = pd.DataFrame({\"No Of Total Values\": df.shape[0] , \"No of NaN values\": df.isnull().sum(),\n",
        "                    \"%age of NaN values\" : round((df.isnull().sum()/ df.shape[0])*100 , 2) })\n",
        "NaN_Checker.sort_values(\"No of NaN values\" , ascending = False)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "director column has highest NaN values 30.7% data is missing\n",
        "\n",
        "cast column has 9% NaN values\n",
        "\n",
        "country , date_added , rating this columns also containing missing values"
      ],
      "metadata": {
        "id": "UjQnELuOYzKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Ploting the null values present in the dataset"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_nan = df.isna()\n",
        "plot_nan.head(2)"
      ],
      "metadata": {
        "id": "S_FjZGQSZE1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = (10 , 5))\n",
        "sns.heatmap(plot_nan)"
      ],
      "metadata": {
        "id": "leFrRfzbZhOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using barplot to check the no of NaN values present in this dataset\n",
        "# null value distribution\n",
        "null_counts = df.isnull().sum()/len(df)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.xticks(np.arange(len(null_counts)),null_counts.index,rotation='vertical')\n",
        "plt.ylabel('fraction of rows with missing data')\n",
        "plt.bar(np.arange(len(null_counts)),null_counts)\n",
        "\n",
        "# director and cast contains large number of null values so we will drop it"
      ],
      "metadata": {
        "id": "X6EUbKHWZ0-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping irrelevent features\n",
        "df.drop(['director','cast'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "qU7wFP6oaEuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NaN values on data_added\n",
        "data_added_NaN = df[df['date_added'].isna()]\n",
        "data_added_NaN.head(2)"
      ],
      "metadata": {
        "id": "cpQj3KU9b6Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_added_NaN.shape"
      ],
      "metadata": {
        "id": "oHuCTXC1cU8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are only 10 observations which are containing NaN values in data_added column\n",
        "print(f\"Before dropping the NaN values from date_added the shape was {df.shape}\")\n",
        "df.dropna(subset = [ 'date_added' ], inplace = True)\n",
        "print(f\"After dropping the NaN values from date_added now the shape is {df.shape}\")"
      ],
      "metadata": {
        "id": "TudQBiO1cedS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for unique values\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "PILnSa9bdEuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unique values of type column\n",
        "df['type'].unique()"
      ],
      "metadata": {
        "id": "AH6ORmfBdYYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Production Growth based on type of the content & release_year\n",
        "yearly_movies=df[df.type =='TV Show']['release_year'].value_counts().sort_index(ascending=False).head(15)\n",
        "yearly_shows=df[df.type =='Movie']['release_year'].value_counts().sort_index(ascending=False).head(15)\n",
        "total_content=df['release_year'].value_counts().sort_index(ascending=False).head(15)\n",
        "yearly_movies.head()"
      ],
      "metadata": {
        "id": "APVDjxSTdf3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale=1.4)\n",
        "total_content.plot(figsize=(12, 6), linewidth=2.5, color='green',label=\"Total Content / year\")\n",
        "yearly_movies.plot(figsize=(12, 6), linewidth=2.5, color='maroon',label=\"Movies / year\",ms=3)\n",
        "yearly_shows.plot(figsize=(12, 6), linewidth=2.5, color='blue',label=\"TV Shows / year\")\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.legend()\n",
        "plt.title(\"Production Growth Yearly\", y=1.02, fontsize=22);"
      ],
      "metadata": {
        "id": "GrNyRCEddrOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # release_year\n",
        " # all unique values present in release_year\n",
        " df['release_year'].unique()"
      ],
      "metadata": {
        "id": "t_eHzqOEd2mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the Datatype of release_year column\n",
        "type(df['release_year'][0])"
      ],
      "metadata": {
        "id": "EVGnQGGVeHKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# value_count is on release_year\n",
        "df['release_year'].value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "3uXY41p5edub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking outliers on release_year column\n",
        "sns.boxplot(x= df.release_year)\n",
        "\n",
        "# As we have seen earlier before 2014 the production growth for Movies & Tv Shows were very less ,that's why it's showing those values(release_year less than 2009) as outliers"
      ],
      "metadata": {
        "id": "U09pYVPGemAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.release_year[0])"
      ],
      "metadata": {
        "id": "qug6J3VhfwHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing outliers with mean value\n",
        "release_year_Q1 = df.release_year.quantile(0.25)\n",
        "release_year_Q3 = df.release_year.quantile(0.75)\n",
        "release_year_IQR = release_year_Q3 - release_year_Q1\n",
        "print(f'release_year_Q1 = {release_year_Q1}\\nrelease_year_Q3 = {release_year_Q3}\\nrelease_year_IQR = {release_year_IQR}')"
      ],
      "metadata": {
        "id": "fAHmb024gENy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we don't have have any release_year which is greater than 2018\n",
        "release_year_outliers = df[(df.release_year < (release_year_Q1 - 1.5 * release_year_IQR)) |\n",
        "                          ( df.release_year > (release_year_Q3 + 1.5 * release_year_IQR)) ]"
      ],
      "metadata": {
        "id": "IpiIhv29gQlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "release_year_outliers"
      ],
      "metadata": {
        "id": "JwsSS7qKgZDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15% value is 2009\n",
        "df[\"release_year\"] = np.where(df[\"release_year\"] <2009, df.release_year.mean(),df['release_year'])"
      ],
      "metadata": {
        "id": "DxH7XXCPg8zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot for release_year\n",
        "# stastical data\n",
        "df.release_year.describe()"
      ],
      "metadata": {
        "id": "kPGl1o8VhD0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x= df.release_year)"
      ],
      "metadata": {
        "id": "ok37Db1ThcOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Datatype of release_year = \",type(df.release_year.iloc[0]))\n",
        "df.release_year = df.release_year.astype(\"int64\")\n",
        "print(f\"Datatype of release_year = \",type(df.release_year.iloc[0]))"
      ],
      "metadata": {
        "id": "T5xwFbwyFw1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Title**"
      ],
      "metadata": {
        "id": "fXB_ABt-F6EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No of unique title present in title column\n",
        "df.title.nunique()"
      ],
      "metadata": {
        "id": "pbknCgWfF85p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape[0]"
      ],
      "metadata": {
        "id": "QFf6NnLgGICr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All the values present in Title are unique\n",
        "#subsetting df\n",
        "df_wordcloud = df['title']\n",
        "text = \" \".join(word for word in df_wordcloud)\n",
        "# Create stopword list:\n",
        "stopwords = set(STOPWORDS)\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"black\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BVPoiKcEGPID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference:\n",
        "\n",
        "It seems like words like \"Love\", \"Man\", \"World\", \"Story\" , \"Christmas\" are very common in titles.\n",
        "\n",
        "I have suprised to see \"Christmas\" ocuured so many time . The reason maybe those movies released on the month of december, but I don't have any information about the release month of movies that's why I am not able to check my hypothesis."
      ],
      "metadata": {
        "id": "7AFa2uPWG3tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Countries producing most number of contents\n",
        "#type\n",
        "#Plotting pie chart on type feature\n",
        "plt.figure(figsize=(10, 5))\n",
        "labels=['TV Show', 'Movie']\n",
        "plt.pie(df['type'].value_counts().sort_values(),labels=labels,explode=[0.01,0.01],\n",
        "        autopct='%1.2f%%', startangle=90)\n",
        "plt.title('Type of Netflix Content')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "76thu-QaG_Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the contents are Movies\n",
        "\n",
        "Less than ⅓ content are Tv Shows"
      ],
      "metadata": {
        "id": "5eLlsADlHSc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# duration\n",
        "# Checking NaN values\n",
        "df.duration.isna().sum()                  # There is no NaN value present."
      ],
      "metadata": {
        "id": "LiwM8FS-HZPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Checking datatype\n",
        "type(df.duration.iloc[0][0])"
      ],
      "metadata": {
        "id": "nInIv8fCHqSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many unique values present in duration column ??\n",
        "df.duration.nunique()"
      ],
      "metadata": {
        "id": "sA5CuHDLHzOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Using value_count() method\n",
        " df.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "mrmlCxziH8N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define convert_seasons_to_min\n",
        "def convert_seasons_to_min(value):\n",
        "  \"\"\"\n",
        "  This function will calculate no of total mins as per season no.\n",
        "  Here our assumptions are\n",
        "    1. on average 5 episodes are there in a season.\n",
        "    2. each episode avg time is 55 mins.\n",
        "  \"\"\"\n",
        "  no_of_avg_episode = 5\n",
        "  if \"Seasons\" in value:\n",
        "    #containing more than 1 seasons\n",
        "    value = value.replace(\"Seasons\",'')\n",
        "    value = value.replace(\" \",\"\")\n",
        "    total_seasons = int(value)\n",
        "    each_season_mins = ( no_of_avg_episode * 55 )\n",
        "    total_mins = (total_seasons * each_season_mins)\n",
        "    return total_mins\n",
        "\n",
        "  elif \"Season\" in value:\n",
        "    # containing only 1 season\n",
        "    value = value.replace(\"Season\",'')\n",
        "    value = value.replace(\" \",\"\")\n",
        "    total_mins = (no_of_avg_episode * 55)\n",
        "    return total_mins"
      ],
      "metadata": {
        "id": "PNZ62E9ZIK8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the function\n",
        "convert_seasons_to_min(\"4 Seasons\")"
      ],
      "metadata": {
        "id": "-RZnIBP_IRl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"4 Seasons\" :\n",
        "\n",
        "4 Seasons = (45) or 20 episodes\n",
        "\n",
        "Each episode avg. time is 55 mins.\n",
        "\n",
        "Total time (in minutes. ) = (5520) min\n",
        "= 1100 mins"
      ],
      "metadata": {
        "id": "epYjsY9sIVpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define all_the_duration_in_minutes\n",
        "def all_the_duration_in_minutes():\n",
        "  \"\"\"\n",
        "  This function will convert all the duration\n",
        "  whether it's in minutes or season format to minute\n",
        "  \"\"\"\n",
        "  # replaced all the min with null string\n",
        "  df['duration'] = df.duration.str.replace(\" min\" , \"\")\n",
        "  # this time_list will contain all the value\n",
        "  time_list =[]\n",
        "  for time in df.duration.values:\n",
        "    if \"Season\" in time:\n",
        "      #time is containing Season\n",
        "      # calling convert_seasons_to_min function to convert\n",
        "      # season to total min\n",
        "      time = convert_seasons_to_min(time)\n",
        "    else:\n",
        "      #replacing single space with \"\"\n",
        "      time = time.replace(\" \",\"\")\n",
        "    #appending time (it's not containing words like min or seasons)\n",
        "    time_list.append(time)\n",
        "\n",
        "  #converting all the time into integer format\n",
        "  time_list = [ int(Time) for Time in time_list]\n",
        "\n",
        "  #Assigning time_list to df.duration\n",
        "  df.duration = time_list"
      ],
      "metadata": {
        "id": "XSB7dAoiIcdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "fjukPR78IntH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_the_duration_in_minutes()"
      ],
      "metadata": {
        "id": "dGltUXIEItov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "OQxyeT7VIwOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis on the duration of the movies\n",
        "sns.set(style=\"darkgrid\")\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.kdeplot(data = df.duration[df['type'] == 'Movie'] , shade=True)"
      ],
      "metadata": {
        "id": "lhOH40zNI2Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most content are about 70 to 120 min duration for movies"
      ],
      "metadata": {
        "id": "GLS8Jvc6JABC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis on the duration of the TV-Shows\n",
        "df['type'].value_counts()"
      ],
      "metadata": {
        "id": "Ku2l_U7uJD1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.kdeplot(data = df.duration[df['type'] == 'TV Show'] , shade=True)"
      ],
      "metadata": {
        "id": "myVhllCPJLR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listed_in\n",
        " # How many unique values present in listed_in ??\n",
        "df.listed_in.nunique()"
      ],
      "metadata": {
        "id": "9W_n3gdpJS3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many NaN values present in listed_in ?\n",
        "df.listed_in.isna().sum()"
      ],
      "metadata": {
        "id": "s7Bn7sEvJsGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value_counts()\n",
        "df.listed_in.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "vnGO6gMNJ1q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Categories\n",
        "categories = \", \".join(df['listed_in']).split(\", \")\n",
        "categories[:5]"
      ],
      "metadata": {
        "id": "NCbiLysDJ-ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(categories)"
      ],
      "metadata": {
        "id": "O0S8Uq6mKGwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(categories))"
      ],
      "metadata": {
        "id": "s_QX5JSnKJVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 42 unique categories present & in this dataset all the categories occured in total 17051 times"
      ],
      "metadata": {
        "id": "hQuUllv3KQOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a dictionary ( category_wise_count )\n",
        "where for each category there will be a value which basically tells us how many times that particular category occured"
      ],
      "metadata": {
        "id": "k0Fqwhw8LKq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_wise_count = {}\n",
        "for category in set(categories):\n",
        "  category_wise_count[category] = categories.count(category)\n",
        "\n",
        "category_wise_count"
      ],
      "metadata": {
        "id": "yw3VEHwkLPLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting category_wise_count by value\n",
        "sorted_category_wise_count = sorted(category_wise_count.items(), key=lambda x: x[1])\n",
        "sorted_category_wise_count[:4]"
      ],
      "metadata": {
        "id": "5GrBu_VoLW2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 most occurred category\n",
        "sorted_category_wise_count[-5:]"
      ],
      "metadata": {
        "id": "iZghAdvtLeSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 most occurred categories\n",
        "top_10_most_occurred_categories = sorted_category_wise_count[-10:]\n",
        "top_10_most_occurred_categories"
      ],
      "metadata": {
        "id": "u_2wOKfzLnoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_most_occurred_category_name = []\n",
        "top_10_most_occurred_category_count = []\n",
        "for tup in top_10_most_occurred_categories:\n",
        "  top_10_most_occurred_category_name.append(tup[0])\n",
        "  top_10_most_occurred_category_count.append(tup[1])\n",
        "\n",
        "top_10_most_occurred_category_name"
      ],
      "metadata": {
        "id": "E0fcPcX2LybL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.description.iloc[0]"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First_des = df.description.iloc[0]\n",
        "First_des"
      ],
      "metadata": {
        "id": "sd4m8H1eSS91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "1U9iFXz-SgS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "vbXBdWf9Smd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Removing punctuations\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space,\n",
        "    # which in effect deletes the punctuation marks\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply(remove_punctuation)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JH6UOaxFTJpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "I4NUWo_RTOPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the stopwords from nltk library\n",
        "sw = nltk.corpus.stopwords.words('english')\n",
        "# displaying the stopwords\n",
        "for i in sw:\n",
        "  print(i , end=',  ')"
      ],
      "metadata": {
        "id": "HU2_xISMTXaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of stopwords in english : \", len(sw))"
      ],
      "metadata": {
        "id": "mpTWz_idTbGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    #Method 1\n",
        "    text1 = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text1)"
      ],
      "metadata": {
        "id": "tEsNJSQbTg9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply( remove_stopwords )\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jpApe6qaTj0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all the values of description are punctutation free ans stopword free"
      ],
      "metadata": {
        "id": "TKLLaxamTsi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using CountVectorizer() to count vocabulary items\n",
        "\n",
        "# Create a count vectorizer object\n",
        "count_vectorizer = CountVectorizer()\n",
        "# fit the count vectorizer using the text data\n",
        "count_vectorizer.fit(df['description'])\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = count_vectorizer.vocabulary_.items()"
      ],
      "metadata": {
        "id": "RNc_xatWTtoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary"
      ],
      "metadata": {
        "id": "epQK-SHLT5NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [ ]\n",
        "count_of_vocab = []\n",
        "for key , value in dictionary:\n",
        "  vocab.append( key )\n",
        "  count_of_vocab.append( value )"
      ],
      "metadata": {
        "id": "PEHOWYAdT8IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DataFrame vocab_before_stemming\n",
        "\n",
        "# Store the count in panadas dataframe with vocab as index\n",
        "vocab_before_stemming = pd.DataFrame({\"Word\": vocab ,\n",
        "                                      \"count\" :count_of_vocab})\n",
        "# Sort the dataframe\n",
        "vocab_before_stemming = vocab_before_stemming.sort_values(\"count\" ,ascending=False)\n",
        "vocab_before_stemming.head(4)"
      ],
      "metadata": {
        "id": "0kXmBnbdUCGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_before_stemming.head(20).T"
      ],
      "metadata": {
        "id": "Fp1Q8jf-UQSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_before_stemming.tail(4)"
      ],
      "metadata": {
        "id": "TdcIL2M4UV3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 - TOP 10 Most Occurred Category By Count\n",
        "plt.figure( figsize = (10,5))\n",
        "color=['lightpink', 'violet', 'green', 'blue', 'cyan' , \"lightblue\" ,'red', 'pink', 'yellow', 'orange']\n",
        "plt.barh(top_10_most_occurred_category_name , top_10_most_occurred_category_count ,\n",
        "        color= color)\n",
        "plt.title(\"Top 10 Most Popular Categories\",fontsize = 19)\n",
        "plt.xlabel(\"Count\", fontsize = 14 )\n",
        "plt.ylabel(\"Category Name\" , fontsize = 14 )\n",
        "plt.figure( figsize = (10,5))"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column no_of_category\n",
        "# Datatype of listed_in values\n",
        "type(df.listed_in.iloc[0])"
      ],
      "metadata": {
        "id": "g2SU9nowOg90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.listed_in.iloc[0])"
      ],
      "metadata": {
        "id": "-W3hSw6JOslm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.listed_in.iloc[0]).split(\",\")"
      ],
      "metadata": {
        "id": "dWWEe9QfOxb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len((df.listed_in.iloc[0]).split(\",\"))"
      ],
      "metadata": {
        "id": "tm7VKcsAO04V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_category = []\n",
        "for categories in df.listed_in.values:\n",
        "  len_categories = len(categories.split(\",\"))\n",
        "  no_of_category.append(len_categories)\n",
        "df['no_of_category'] = no_of_category"
      ],
      "metadata": {
        "id": "ojilHpodO4kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['listed_in' , 'no_of_category']].head()"
      ],
      "metadata": {
        "id": "nuDdu7CvO8le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 - Histogram of no_of_category using listed_in\n",
        "df.no_of_category.unique()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.no_of_category.value_counts()"
      ],
      "metadata": {
        "id": "XHU3PromPIsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.hist(df.no_of_category , bins=[1,2,3,4] , range = (1 ,4) , rwidth = 0.85, color ='lightblue')\n",
        "plt.xlabel(\"No of categories\")\n",
        "plt.ylabel(\"Count\")"
      ],
      "metadata": {
        "id": "ykUhyTA4PNMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 - Most popular TV-Shows Rating\n",
        "df['type'].unique()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tv_show = df[df['type']== 'TV Show' ]\n",
        "df_tv_show.head(2)"
      ],
      "metadata": {
        "id": "WRpn8s--Pht4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pointplot on top tv show ratings\n",
        "tv_ratings = df_tv_show.groupby(['rating'])['show_id'].count().reset_index(name = 'count').sort_values(by = 'count', ascending = False)\n",
        "fig_dims = (7,4)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('Top TV Show Ratings Based On Rating System',size='15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MmlARQazPmy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 - Most popular Movies Rating\n",
        "df_movies = df[df['type'] == 'Movie' ]\n",
        "df_movies.head(2)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pointplot on top tv show ratings\n",
        "tv_ratings = df_movies.groupby(['rating'])['show_id'].count().reset_index(name = 'count').sort_values(by = 'count', ascending = False)\n",
        "fig_dims = (10,5)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('Top Movie Ratings Based On Rating System',size='20')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SvV8s-t8RC3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the contents got ratings like\n",
        "\n",
        "TV-MA (For Mature Audiences)\n",
        "TV-14 ( May be unsuitable for children under 14 )\n",
        "TV-PG ( Parental Guidance Suggested )\n",
        "NR ( Not Rated )"
      ],
      "metadata": {
        "id": "76X-7m7bRKPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DataFrame vocab_before_stemming\n",
        "# Store the count in panadas dataframe with vocab as index\n",
        "vocab_before_stemming = pd.DataFrame({\"Word\": vocab ,\n",
        "                                      \"count\" :count_of_vocab})\n",
        "# Sort the dataframe\n",
        "vocab_before_stemming = vocab_before_stemming.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "3Q0Eus6GR09c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 - TOP 10 most occurred words\n",
        "top15_most_ocurred_vacab = vocab_before_stemming.head(15)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words = top15_most_ocurred_vacab.Word.values\n",
        "top15_most_occurred_words"
      ],
      "metadata": {
        "id": "wPXo4lgpUiy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words_count = top15_most_ocurred_vacab['count'].values\n",
        "top15_most_occurred_words_count"
      ],
      "metadata": {
        "id": "f4C8GiYzUlmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 10,5 ))\n",
        "plt.xlim(19550, 19600)\n",
        "plt.barh(top15_most_occurred_words , top15_most_occurred_words_count )"
      ],
      "metadata": {
        "id": "ja_DqFyDUoY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now will use SnowballStemmer( 'english' )\n",
        "# Create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "C-091Zu3U5gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Apply_stemming(text):\n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text.split()]\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "SRGiwsGnVAMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming for description\n",
        "df['description'] = df['description'].apply( Apply_stemming )\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PYoBdL0QVDUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now again will use TfidfVectorizer (after stemming)\n",
        "# Create the object of tfid vectorizer\n",
        "tfid_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer using the text data\n",
        "tfid_vectorizer.fit(df['description'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = tfid_vectorizer.vocabulary_.items()"
      ],
      "metadata": {
        "id": "8_Ar_720VQ8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store the vocab and counts\n",
        "vocab = []\n",
        "count_of_vocab = []\n",
        "# Iterate through each vocab and count append the value to designated lists\n",
        "for key, value in dictionary:\n",
        "    vocab.append(key)\n",
        "    count_of_vocab.append(value)"
      ],
      "metadata": {
        "id": "hFh8jaO5VYco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 -Creating a new DataFrame vocab_after_stemming\n",
        "# Store the count in panadas dataframe with vocab as index\n",
        "vocab_after_stemming = pd.DataFrame({\"Word\": vocab ,\n",
        "                                      \"count\" :count_of_vocab})\n",
        "# Sort the dataframe\n",
        "vocab_after_stemming = vocab_after_stemming.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_vocab = vocab_after_stemming.head(15)\n",
        "top15_most_occurred_words = top15_most_ocurred_vocab.Word.values\n",
        "top15_most_occurred_words"
      ],
      "metadata": {
        "id": "nHMcp6AVVmO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words_count = top15_most_ocurred_vocab['count'].values\n",
        "top15_most_occurred_words_count"
      ],
      "metadata": {
        "id": "ej4hKCMwVt0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 10,5 ))\n",
        "plt.xlim(14225, 14241)\n",
        "plt.barh(top15_most_occurred_words , top15_most_occurred_words_count)"
      ],
      "metadata": {
        "id": "oPKVG__oV0Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column length which will contain length of description\n",
        "df['Length(description)'] = df['description'].apply(lambda x: len(x))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "w1_aUwUmWDAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.description.iloc[0])"
      ],
      "metadata": {
        "id": "sYGz7GtfWQ0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # listed_in\n",
        " # Removing punctutations\n",
        " df.columns"
      ],
      "metadata": {
        "id": "8MCl7PL9Wcnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['listed_in'] = df['listed_in'].apply(remove_punctuation)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gYPlT1pAWlje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords\n",
        "#Remove stopwords for listed_in\n",
        "df['listed_in'] = df['listed_in'].apply( remove_stopwords )\n",
        "df.head( 2 )"
      ],
      "metadata": {
        "id": "xGdPLCQKWoj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using CountVectorizer() to count vocabulary items\n",
        "# Create a count vectorizer object\n",
        "count_vectorizer = CountVectorizer()\n",
        "# fit the count vectorizer using the text data\n",
        "count_vectorizer.fit(df['listed_in'])\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = count_vectorizer.vocabulary_.items()\n",
        "dictionary"
      ],
      "metadata": {
        "id": "w0YJVDw7WqVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [ ]\n",
        "count_of_vocab = []\n",
        "for key , value in dictionary:\n",
        "  vocab.append( key )\n",
        "  count_of_vocab.append( value )"
      ],
      "metadata": {
        "id": "0e8gH6C5W9v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listed_in_vocab_before_stem = pd.DataFrame({\"Word\": vocab , \"count\" :count_of_vocab})\n",
        "\n",
        "listed_in_vocab_before_stem = listed_in_vocab_before_stem.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "iVWYQCQZXCu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listed_in_vocab_before_stem.head()"
      ],
      "metadata": {
        "id": "N2krEisTXFND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listed_in_vocab_before_stem.tail()"
      ],
      "metadata": {
        "id": "PnZ6lOhvXH9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 - TOP 10 most occurred words in listed in\n",
        "top15_most_ocurred_vocab_listed_in = listed_in_vocab_before_stem.head(15)\n",
        "top15_most_ocurred_words_listed_in = top15_most_ocurred_vocab_listed_in.Word.values\n",
        "top15_most_ocurred_words_listed_in\n",
        "top15_most_occurred_words_in_listed_in_count = top15_most_ocurred_vocab_listed_in['count'].values\n",
        "top15_most_occurred_words_in_listed_in_count"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 10,5 ))\n",
        "plt.xlim(25, 42 )\n",
        "plt.barh(top15_most_ocurred_words_listed_in , top15_most_occurred_words_in_listed_in_count )"
      ],
      "metadata": {
        "id": "z0ydP9IkXWf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now will use SnowballStemmer( 'english' )\n",
        "# Stemming for description\n",
        "df['listed_in'] = df['listed_in'].apply( Apply_stemming )\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "jKPACgZZXdD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now will use TfidfVectorizer (after stemming\n",
        "# Create the object of tfid vectorizer\n",
        "tfid_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer using the text data\n",
        "tfid_vectorizer.fit(df['listed_in'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = tfid_vectorizer.vocabulary_.items()\n",
        "dictionary"
      ],
      "metadata": {
        "id": "RKitSkiaXpCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store the vocab and counts\n",
        "vocab = []\n",
        "count_of_vocab = []\n",
        "# Iterate through each vocab and count append the value to designated lists\n",
        "for key, value in dictionary:\n",
        "    vocab.append(key)\n",
        "    count_of_vocab.append(value)"
      ],
      "metadata": {
        "id": "OAAd6W7sX0YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DataFrame vocab_after_stemming_listed_in\n",
        "vocab_after_stemming_listed_in = pd.DataFrame({\"Word\": vocab , \"count\" :count_of_vocab})\n",
        "# Sort the dataframe by count\n",
        "vocab_after_stemming_listed_in = vocab_after_stemming_listed_in.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "zuqcTqonX7oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_vocab_lised_in_after_stem = vocab_after_stemming_listed_in.head(15)\n",
        "top15_most_ocurred_vocab_lised_in_after_stem_word = top15_most_ocurred_vocab_lised_in_after_stem.Word.values\n",
        "top15_most_ocurred_vocab_lised_in_after_stem_word"
      ],
      "metadata": {
        "id": "uVQzphYDYDwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words_listed_in_count = top15_most_ocurred_vocab_lised_in_after_stem['count'].values\n",
        "top15_most_occurred_words_listed_in_count"
      ],
      "metadata": {
        "id": "zxzf2UbjYIuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 - top vocab present in listed_in (after stemming)\n",
        "plt.figure( figsize = ( 10,5 ))\n",
        "plt.xlim(25, 40 )\n",
        "plt.barh(top15_most_ocurred_vocab_lised_in_after_stem_word , top15_most_occurred_words_listed_in_count )"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column length( listed-in ) which will contain length of listed_in\n",
        "df['Length(listed-in)'] = df['listed_in'].apply(lambda x: len(x))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "0Bzb0hRJYdak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "lC518NmCYk9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['description', 'Length(description)', 'listed_in' ,'Length(listed-in)' ]].head(3)"
      ],
      "metadata": {
        "id": "HgRPx81oYnDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_features_rec = df[['no_of_category' ,'Length(description)','Length(listed-in)']]\n",
        "stdscaler = preprocessing.StandardScaler()\n",
        "X_features_rec.describe()"
      ],
      "metadata": {
        "id": "l0e1pxoWq2M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_rescale=stdscaler.fit_transform(X_features_rec)\n",
        "X=X_rescale\n",
        "silhouette_score_ = [  ]\n",
        "range_n_clusters = [i for i in range(2,16)]"
      ],
      "metadata": {
        "id": "qveAyg-gtTrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    silhouette_score_.append([int(n_clusters) , round(score , 3)])\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "154apZ0PtWKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = pd.DataFrame(silhouette_score_ , columns = [\"n clusters\" , \"silhouette score\"])\n",
        "temp = temp.sort_values( \"silhouette score\" , ascending = False )\n",
        "temp.head(14)"
      ],
      "metadata": {
        "id": "3SdzExuWtdI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE :-\n",
        "The value of the silhouette coefﬁcient is between [-1, 1]. A score of 1 denotes the best meaning that the data point i is very compact within the cluster to which it belongs and far away from the other clusters. The worst value is -1. Values near 0 denote overlapping clusters"
      ],
      "metadata": {
        "id": "eitQsYnctk62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_n_clusters = [i for i in range(2,16)]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) /n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6xc9rhqCtovG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters. To calculate the Silhouette score for each observation/data point, the following distances need to be found out for each observations belonging to all the clusters:\n",
        "\n",
        "Mean distance between the observation and all other data points in the same cluster. This distance can also be called a mean intra-cluster distance. The mean distance is denoted by a.\n",
        "Mean distance between the observation and all other data points of the next nearest cluster. This distance can also be called a mean nearest-cluster distance. The mean distance is denoted by b.\n",
        "\n",
        "The Silhouette Coefficient for a sample is  S=(b−a)/max(a,b) ."
      ],
      "metadata": {
        "id": "ZnndAIwguCbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 - Elbow Method\n",
        "\n",
        "sum_of_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
        "    km = km.fit(X)\n",
        "    sum_of_sq_dist[k] = km.inertia_\n",
        "\n",
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 - Will be using 3 clusters\n",
        "kmeans = KMeans(n_clusters = 3 )\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)\n",
        "\n",
        "plt.figure(figsize=(10 , 6))\n",
        "plt.title('description and listed_in')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='spring')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11- DBSCAN\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X)\n",
        "plt.figure(figsize=( 8 , 5 ))\n",
        "plt.scatter(X[:,1], X[:,2], c=y_pred)           # The black colour dots(*) are nois"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12- Dendrogram\n",
        "# Let's import sch\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(10,5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'single'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Content')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of clusters will be the number of vertical lines which are being intersected by the line drawn using the threshold\n",
        "\n",
        "No. of Cluster = 3"
      ],
      "metadata": {
        "id": "ls_Y-bsovOjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 - Agglomerative Clustering\n",
        "# Let's  import AgglomerativeClustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (three dimensions only)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = '1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = '2')\n",
        "# plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = '3')\n",
        "\n",
        "plt.title('Clusters of content')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OkfLfQLWvdRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "1. Director and cast contains a large number of null values so we will drop these 2 columns.\n",
        "\n",
        "2. In this dataset there are two types of contents where 30.86% includes TV shows and the remaining 69.14% carries Movies.\n",
        "\n",
        "3. We have reached a conclusion from our analysis from the content added over years that Netflix is focusing movies and TV shows (Fom 2016 data we get to know that Movies is increased by 80% and TV shows is increased by 73% compare)\n",
        "\n",
        "4. From the dataset insights we can conclude that the most number of TV Shows released in 2017 and for Movies it is 2020.\n",
        "\n",
        "5. On Netflix USA has the largest number of contents. And most of the countries preferred to produce movies more than TV shows.\n",
        "\n",
        "6. Most of the movies are belonging to 3 categories\n",
        "7. TOP 3 content categories are International movies , dramas , comedies.\n",
        "8. In text analysis (NLP) I used stop words, removed punctuations , stemming & TF-IDF vectorizer and other functions of NLP.\n",
        "\n",
        "9. Applied different clustering models like Kmeans, hierarchical, Agglomerative clustering, DBSCAN on data we got the best cluster arrangements.\n",
        "\n",
        "10. By applying different clustering algorithms to our dataset .we get the optimal number of cluster is equal to 3Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}